name: Showrooms Scraper

on:
  schedule:
    - cron: '0 5 1 */3 *' # Run at 05:00 UTC on the 1st day of every 3rd month (8:00 AM in Kuwait)
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape2:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # Reduced to 6 hours (GitHub max)

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright
        run: |
          playwright install --with-deps chromium

      - name: Create Service Account Key File
        run: |
          echo '${{ secrets.GCLOUD_KEY_JSON }}' > service_account.json
        shell: bash

      - name: Run the scraper
        env:
          SHOWROOMS_GCLOUD_KEY_JSON: ${{ secrets.GCLOUD_KEY_JSON }}
        run: |
          python oogoo_showrooms.py

      - name: Upload Logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: scraper-logs
          path: |
            *.log
            *.xlsx
